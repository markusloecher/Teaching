[["index.html", "Build your own neural network in 96 mins Course Outline", " Build your own neural network in 96 mins Aparna Menon Loecher Course Outline Who will benefit from this course ? This series of 8 videos - which add to a total of 96 minutes in length- is for anyone who wants to seriously understand the inner workings of neural networks without being overwhelmed by mathematical notation and unreasonable prerequisites in computer science competencies. The course slowly develops the main ideas and implements them in python code right away; I believe in learning by coding. I do not take shortcuts when it comes to concepts and mathematical details but yet assume that you do not know much more than high school math and basic statistical intuition. Only our dive into python does assume a prior familiarity with programming in general and python in particular. I feel that there is an abundance of great resources to learn coding, which I would not want to replicate. This course is open to anyone! How is this course different from other online NN tutorials? Explain the structure to me 1. Introduction and Motivation In part 1, I initially set the stage (1) by introducing the building blocks of classifiers, in particular Boolean logic, thresholds and activation functions, leading to the sigmoid function with multiple inputs as a highly simplified model of a neuron. I then combine many of these sigmoidal units (nodes) into a network consisting of muliple layers. 2. Forward Propagation After this high level motivation, the second part begins with an (optional) review of matrix multiplication (2.1), followed by the basic mechanism of information flow (forward propagation) in a tiny neural network consisting of just 2 layers, each with 2 nodes (2.2). The \\(2 \\times 2\\) network is small enough to compute forward propagation by hand. I then use a slightly larger network of 3 layers,3 nodes (2.3) to introduce matrix multiplications as a convenient way to scale up these tedious operations. I conclude the second part by implementing our first neural network code in python (@ref(NN_python)). 3. Backward Propagation The third part tackles the more challenging idea of training a neural network by updating its weights. I apply gradient descent on the weights in each layer sucdessively, starting with the last and working our way backwards to the first. This process is called back propagation (3). I thoroughly explain (3.1) how the errors in a node in layer \\(j\\) are being distributed back to the nodes in the previous layer \\(j-1\\). The third part ends with a matrix view of back propagation (3.2). The following two chapters are extras, not included in the 96 minutes. 4. Case Study: MNIST MNIST (Modified National Institute of Standards and Technology) is the de facto hello world dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike. In section (4), I put our newly written neural network code to test how well we can identify digits from a dataset of tens of thousands of handwritten images. 5. Deep Learning Of course, our home made neural network code could not possibly compete with the existing libraries such as pytorch, tensorflow and keras which are much more mature, robust, efficient. The main reason, I think it worthwhile to write your own python code is pedagogy: I believe that a lot of the mystery of NNs disappears and have witnessed a significant boost on the learning curve for most students. I (obviously) have covered only the simplest possible architecture of a neural net. For more serious image processing, text classification and other NLP type tasks, the most competetive architectures are convolutional (CNNs), recurrent as well as Long short-term memory (LTSM) neural networks. To get you started on building your first neural network with keras, section (5) shows an example of MNIST digit classification. "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction We introducing the building blocks of classifiers, in particular Boolean logic, thresholds and activation functions, leading to the sigmoid function with multiple inputs as a highly simplified model of a neuron. We then combine many of these sigmoidal units (nodes) into a network consisting of muliple layers. "],["ForwardProp.html", "Chapter 2 Forward Propagation 2.1 Matrix Multiplication 2.2 2 layers, 2 nodes 2.3 3 layers, 3 nodes 2.4 Python Code 2.5 Actual Code 2.6 Exercises", " Chapter 2 Forward Propagation 2.1 Matrix Multiplication 2.2 2 layers, 2 nodes 2.3 3 layers, 3 nodes 2.4 Python Code 2.5 Actual Code The following code excerpt is taken from the MYONN github page # python notebook for Make Your Own Neural Network # (c) Tariq Rashid, 2016 # license is GPLv2 import numpy # scipy.special for the sigmoid function expit() import scipy.special 2.5.1 Defining the NN classes We have saved the following code in a separate file neuralNetwork.py (for later easy import). # neural network class definition class neuralNetwork: # initialise the neural network def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate): # set number of nodes in each input, hidden, output layer self.inodes = inputnodes self.hnodes = hiddennodes self.onodes = outputnodes # link weight matrices, wih and who # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer # w11 w21 # w12 w22 etc self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes)) self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes)) # learning rate self.lr = learningrate # activation function is the sigmoid function self.activation_function = lambda x: scipy.special.expit(x) pass # train the neural network def train(self, inputs_list, targets_list): # convert inputs list to 2d array inputs = numpy.array(inputs_list, ndmin=2).T targets = numpy.array(targets_list, ndmin=2).T # calculate signals into hidden layer hidden_inputs = numpy.dot(self.wih, inputs) # calculate the signals emerging from hidden layer hidden_outputs = self.activation_function(hidden_inputs) # calculate signals into final output layer final_inputs = numpy.dot(self.who, hidden_outputs) # calculate the signals emerging from final output layer final_outputs = self.activation_function(final_inputs) # output layer error is the (target - actual) output_errors = targets - final_outputs # hidden layer error is the output_errors, split by weights, recombined at hidden nodes hidden_errors = numpy.dot(self.who.T, output_errors) # update the weights for the links between the hidden and output layers self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs)) # update the weights for the links between the input and hidden layers self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs)) pass # query the neural network def query(self, inputs_list): # convert inputs list to 2d array inputs = numpy.array(inputs_list, ndmin=2).T # calculate signals into hidden layer hidden_inputs = numpy.dot(self.wih, inputs) # calculate the signals emerging from hidden layer hidden_outputs = self.activation_function(hidden_inputs) # calculate signals into final output layer final_inputs = numpy.dot(self.who, hidden_outputs) # calculate the signals emerging from final output layer final_outputs = self.activation_function(final_inputs) return final_outputs We initialize a small \\(3 \\times 3\\) network and query it (no training yet): # number of input, hidden and output nodes input_nodes = 3 hidden_nodes = 3 output_nodes = 3 # learning rate is 0.3 learning_rate = 0.3 # create instance of neural network numpy.random.seed(123) n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate) # test query (doesn&#39;t mean anything useful yet) n.query([1.0, 0.5, -1.5]) ## array([[0.44682114], ## [0.56514072], ## [0.52383514]]) 2.6 Exercises to come "],["BackwardProp.html", "Chapter 3 Backward Propagation 3.1 Splitting the error 3.2 Matrix View of Gradient Descent 3.3 Exercises", " Chapter 3 Backward Propagation 3.1 Splitting the error 3.2 Matrix View of Gradient Descent 3.3 Exercises to come "],["CaseStudy.html", "Chapter 4 Case Study: MNIST 4.1 Train and Test Sets 4.2 Train and test the neural network 4.3 Optimizing 4.4 Exercises", " Chapter 4 Case Study: MNIST We are now going to classify handwritten digits from the MNIST database using our developed Python code. # python notebook for Make Your Own Neural Network # code for a 3-layer neural network, and code for learning the MNIST dataset # (c) Tariq Rashid, 2016 # license is GPLv2 We are going to reuse the neuralNetwork class written in python in section (@ref(NN_python_code)) which is defined in the separate file neuralNetwork.py. import neuralNetwork # library for plotting arrays import matplotlib.pyplot # number of input, hidden and output nodes input_nodes = 784 hidden_nodes = 200 output_nodes = 10 # learning rate learning_rate = 0.1 # create instance of neural network n = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate) 4.1 Train and Test Sets We will use these datasets: - A training set http://www.pjreddie.com/media/files/mnist_train.csv - A test set http://www.pjreddie.com/media/files/mnist_test.csv The first number is the label, and the rest of the 784 numbers are the colour values for the pixels that make up the image. These colour values range between 0 and 255. import pandas as pd nrows= 1000 # load the mnist training data CSV file into a list training_data = pd.read_csv(&quot;mnist_dataset/mnist_train.csv.gz&quot;, header=None, nrows= nrows) test_data = pd.read_csv(&quot;mnist_dataset/mnist_test.csv.gz&quot;, header=None, nrows= nrows) trainDim = training_data.shape #training_data.head() 4.2 Train and test the neural network # test the neural network def test_NN(test_data): # scorecard for how well the network performs, initially empty scorecard = [] testDim = test_data.shape # go through all the records in the test data set for i in numpy.arange(testDim[0]): # correct answer is first value correct_label = int(test_data.iloc[i,0]) # scale and shift the inputs inputs = (numpy.asfarray(test_data.iloc[i,1:]) / 255.0 * 0.99) + 0.01 # query the network outputs = n.query(inputs) # the index of the highest value corresponds to the label label = numpy.argmax(outputs) # append correct or incorrect to list if (label == correct_label): # network&#39;s answer matches correct answer, add 1 to scorecard scorecard.append(1) else: # network&#39;s answer doesn&#39;t match correct answer, add 0 to scorecard scorecard.append(0) pass pass # calculate the performance score, the fraction of correct answers scorecard_array = numpy.asarray(scorecard) return scorecard_array.sum() / scorecard_array.size # train the neural network # epochs is the number of times the training data set is used for training epochs = 10 testAcc = numpy.zeros(epochs) for e in range(epochs): # go through all records in the training data set for i in numpy.arange(trainDim[0]): # scale and shift the inputs inputs = (numpy.asfarray(training_data.iloc[i,1:])/255.0 * 0.99) + 0.01 targets = numpy.zeros(output_nodes) + 0.01 targets[int(training_data.iloc[i,0])] = 0.99 n.train(inputs, targets) pass testAcc[e] = test_NN(test_data) #print(&quot;done with epoch&quot;, e, &quot;, performance = &quot;, testAcc[e]) pass print (&quot;performance = &quot;, testAcc[4]) ## [1] &quot;performance = 0.9712&quot; 4.3 Optimizing 4.3.1 Multiple Epochs 4.3.2 Learning Rate 4.3.3 Number of hidden nodes 4.4 Exercises to come "],["DeepML.html", "Chapter 5 Deep Learning 5.1 Keras 5.2 Exercises", " Chapter 5 Deep Learning 5.1 Keras The keras library greatly simplifies the construction and training of even very complex neural networks. The author states: Keras is the high-level API of TensorFlow 2.0: an approchable, highly-productive interface for solving machine learning problems, with a focus on modern deep learning. It provides essential abstractions and building blocks for developing and shipping machine learning solutions with high iteration velocity. Keras empowers engineers and researchers to take full advantage of the scalability and cross-platform capabilities of TensorFlow 2.0: you can run Keras on TPU or on large clusters of GPUs, and you can export your Keras models to run in the browser or on a mobile device. 5.1.1 Digit Recognition We are borrowing heavily from the 2.1-a-first-look-at-a-neural-network.ipynb notebook which accompanies the fantastic Deep learning with Python book The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. Its a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of solving MNIST as the Hello World of deep learning  its what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on. The MNIST dataset comes pre-loaded in Keras, in the form of a set of four Numpy arrays: from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images and train_labels form the training set, the data that the model will learn from. The model will then be tested on the test set, test_images and test_labels. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels. Lets have a look at the training and test data: print(train_images.shape) print(test_images.shape) Lets build our network from keras import models from keras import layers network = models.Sequential() network.add(layers.Dense(512, activation=&#39;relu&#39;, input_shape=(28 * 28,))) network.add(layers.Dense(10, activation=&#39;softmax&#39;)) The core building block of neural networks is the layer, a data-processing module which you can conceive as a filter for data. Some data comes in, and comes out in a more useful form. Precisely, layers extract representations out of the data fed into them  hopefully representations that are more meaningful for the problem at hand. Most of deep learning really consists of chaining together simple layers which will implement a form of progressive data distillation. A deep learning model is like a sieve for data processing, made of a succession of increasingly refined data filters  the layers. Here our network consists of a sequence of two Dense layers, which are densely-connected (also called fully-connected) neural layers. The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes. To make our network ready for training, we need to pick three more things, as part of compilation step: A loss function: the is how the network will be able to measure how good a job it is doing on its training data, and thus how it will be able to steer itself in the right direction. An optimizer: this is the mechanism through which the network will update itself based on the data it sees and its loss function. Metrics to monitor during training and testing. Here we will only care about accuracy (the fraction of the images that were correctly classified). The exact purpose of the loss function and the optimizer will be made clear later. network.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) Before training, we will preprocess our data by reshaping it into the shape that the network expects, and scaling it so that all values are in the [0, 1] interval. Previously, our training images for instance were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1. train_images = train_images.reshape((60000, 28 * 28)) train_images = train_images.astype(&#39;float32&#39;) / 255 test_images = test_images.reshape((10000, 28 * 28)) test_images = test_images.astype(&#39;float32&#39;) / 255 We also need to categorically encode the labels from keras.utils import to_categorical train_labels = to_categorical(train_labels) test_labels = to_categorical(test_labels) We are now ready to train our network, which in Keras is done via a call to the fit method of the network: we fit the model to its training data. network.fit(train_images, train_labels, epochs=5, batch_size=128) Two quantities are being displayed during training: the loss of the network over the training data, and the accuracy of the network over the training data. We quickly reach an accuracy of 0.989 (i.e. 98.9%) on the training data. Now lets check that our model performs well on the test set too: test_loss, test_acc = network.evaluate(test_images, test_labels) print(&#39;test_acc:&#39;, test_acc) Our test set accuracy turns out to be 97.8%  thats quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of overfitting, the fact that machine learning models tend to perform worse on new data than on their training data. 5.2 Exercises to come "],["solutions.html", "Chapter 6 Solutions To exercises", " Chapter 6 Solutions To exercises "],["references.html", "References", " References 6.0.1 Make Your Own Neural Network This course leans heavily on the wonderful gentle journey through the mathematics of neural networks book and its accompanying github resources. 6.0.2 Deep learning with Python The last chapter with focus on deep networks and the keras library takes its main inspiration from the popular Deep learning with Python book. The author has graciously provided notebooks for all chapters. Recurrent neural networks are nicely explained in the blogs by Terence Parr as well as Andrej Karpathy. "]]
